{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877567a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alvin/miniconda3/envs/cs336_alignment/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 38.18it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import wandb\n",
    "import random\n",
    "import pathlib\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, PreTrainedTokenizerBase\n",
    "\n",
    "from cs336_alignment.data_loading import iterate_batches\n",
    "\n",
    "# Run out of memory running the normal dataset\n",
    "# Create a custom PackedSFTDataset class to load piece of the dataset\n",
    "class PackedSFTDataset(Dataset):\n",
    "    def __init__(self, tokenizer: PreTrainedTokenizerBase, dataset_path: str, seq_length: int, shuffle: bool, max_samples: int = None):\n",
    "        with open(dataset_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            raw_data = [json.loads(line) for line in f]\n",
    "\n",
    "        if shuffle:\n",
    "            random.shuffle(raw_data)\n",
    "\n",
    "        if max_samples is not None:\n",
    "            raw_data = raw_data[:max_samples]\n",
    "\n",
    "        self.inputs = []\n",
    "        self.outputs = []\n",
    "        \n",
    "        for ex in raw_data:\n",
    "            sample = (\n",
    "                \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    "                f\"\\n\\n### Instruction:\\n{ex['prompt']}\\n\\n### Response:\\n{ex['response']}\"\n",
    "            )\n",
    "            tokenized = tokenizer.encode(sample, truncation=True, max_length=seq_length+1)\n",
    "            if len(tokenized) >= 2:\n",
    "                self.inputs.append(tokenized[:-1])\n",
    "                self.outputs.append(tokenized[1:])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = self.inputs[idx]\n",
    "        labels = self.outputs[idx]\n",
    "        \n",
    "        input_ids += [tokenizer.pad_token_id] * (SEQ_LENGTH - len(input_ids))\n",
    "        labels += [-100] * (SEQ_LENGTH - len(labels))  # ignore padding in loss\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "            \"labels\": torch.tensor(labels, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Configs\n",
    "TRAIN_PATH = \"/home/alvin/Homework/s2025-assignment3-alignment/data/tuning/safety_augmented_ultrachat_200k_single_turn/train.jsonl\"\n",
    "DEV_PATH = \"/home/alvin/Homework/s2025-assignment3-alignment/data/tuning/safety_augmented_ultrachat_200k_single_turn/test.jsonl\"\n",
    "MODEL_PATH = \"/home/alvin/Homework/s2025-assignment3-alignment/models/Qwen/Qwen2.5-3B-Instruct\"\n",
    "OUTPUT_DIR = \"./qwen2.5-3B-instruct-finetuned\"\n",
    "PROJECT_NAME = \"EE491B_qwen2.5-3B\"\n",
    "\n",
    "SEQ_LENGTH = 2048\n",
    "BATCH_SIZE = 1\n",
    "GRAD_ACCUMULATION_STEPS = 8\n",
    "TRAIN_STEPS = 100\n",
    "EVAL_INTERVAL = 50\n",
    "EVAL_ITERS = 50\n",
    "LEARNING_RATE = 5e-5\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Initialize model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_PATH, trust_remote_code=True, torch_dtype=torch.float32 if torch.cuda.is_available() else torch.float32)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "# Dataset and Dataloaders\n",
    "train_dataset = PackedSFTDataset(tokenizer, TRAIN_PATH, SEQ_LENGTH, shuffle=True, max_samples=100)\n",
    "dev_dataset = PackedSFTDataset(tokenizer, DEV_PATH, SEQ_LENGTH, shuffle=False, max_samples=10)\n",
    "train_loader = iterate_batches(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "dev_loader = iterate_batches(dev_dataset, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0798d81a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malvinyang101\u001b[0m (\u001b[33malvinyang101-university-of-hawaii-at-manoa\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/alvin/Homework/s2025-assignment3-alignment/notebooks/wandb/run-20250425_235343-t7tbgwda</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/alvinyang101-university-of-hawaii-at-manoa/EE491B_qwen2.5-3B/runs/t7tbgwda' target=\"_blank\">apricot-pond-5</a></strong> to <a href='https://wandb.ai/alvinyang101-university-of-hawaii-at-manoa/EE491B_qwen2.5-3B' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/alvinyang101-university-of-hawaii-at-manoa/EE491B_qwen2.5-3B' target=\"_blank\">https://wandb.ai/alvinyang101-university-of-hawaii-at-manoa/EE491B_qwen2.5-3B</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/alvinyang101-university-of-hawaii-at-manoa/EE491B_qwen2.5-3B/runs/t7tbgwda' target=\"_blank\">https://wandb.ai/alvinyang101-university-of-hawaii-at-manoa/EE491B_qwen2.5-3B/runs/t7tbgwda</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:  99%|█████████▉| 99/100 [51:33<00:31, 31.25s/it]  \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>step</td><td>▁▂▃▃▄▅▅▆▆▇█</td></tr><tr><td>train/loss</td><td>█▄▂▃▂▁▂▁▁▂</td></tr><tr><td>val/loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>step</td><td>90</td></tr><tr><td>train/loss</td><td>4.01274</td></tr><tr><td>val/loss</td><td>4.85122</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">apricot-pond-5</strong> at: <a href='https://wandb.ai/alvinyang101-university-of-hawaii-at-manoa/EE491B_qwen2.5-3B/runs/t7tbgwda' target=\"_blank\">https://wandb.ai/alvinyang101-university-of-hawaii-at-manoa/EE491B_qwen2.5-3B/runs/t7tbgwda</a><br> View project at: <a href='https://wandb.ai/alvinyang101-university-of-hawaii-at-manoa/EE491B_qwen2.5-3B' target=\"_blank\">https://wandb.ai/alvinyang101-university-of-hawaii-at-manoa/EE491B_qwen2.5-3B</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250425_235343-t7tbgwda/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete. Model saved at ./qwen2.5-3B-instruct-finetuned/final_model\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# WandB Logging\n",
    "wandb.init(project=PROJECT_NAME, config={\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"grad_accumulation_steps\": GRAD_ACCUMULATION_STEPS,\n",
    "    \"train_steps\": TRAIN_STEPS,\n",
    "    \"learning_rate\": LEARNING_RATE,\n",
    "    \"model\": MODEL_PATH,\n",
    "})\n",
    "\n",
    "# Training Loop\n",
    "model.train()\n",
    "step = 0\n",
    "optimizer.zero_grad()\n",
    "for epoch in range(100):\n",
    "    pbar = tqdm(train_loader, desc=f\"Training epoch {epoch}\")\n",
    "    for batch in pbar:\n",
    "        batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss / GRAD_ACCUMULATION_STEPS\n",
    "        loss.backward()\n",
    "\n",
    "        if (step + 1) % GRAD_ACCUMULATION_STEPS == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        if step % 10 == 0:\n",
    "            wandb.log({\"train/loss\": loss.item() * GRAD_ACCUMULATION_STEPS, \"step\": step})\n",
    "\n",
    "        if step > 0 and step % EVAL_INTERVAL == 0:\n",
    "            model.eval()\n",
    "            eval_losses = []\n",
    "            with torch.no_grad():\n",
    "                for eval_batch in dev_loader:\n",
    "                    eval_batch = {k: v.to(DEVICE) for k, v in eval_batch.items()}\n",
    "                    outputs = model(**eval_batch)\n",
    "                    eval_losses.append(outputs.loss.item())\n",
    "            val_loss = sum(eval_losses) / len(eval_losses)\n",
    "            wandb.log({\"val/loss\": val_loss, \"step\": step})\n",
    "            model.train()\n",
    "\n",
    "        step += 1\n",
    "\n",
    "        if step >= TRAIN_STEPS:\n",
    "            break\n",
    "    if step >= TRAIN_STEPS:\n",
    "        break\n",
    "\n",
    "# Save model\n",
    "save_path = os.path.join(OUTPUT_DIR, \"final_model\")\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "wandb.finish()\n",
    "\n",
    "print(f\"Training complete. Model saved at {save_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs336_alignment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
