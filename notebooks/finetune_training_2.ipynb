{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877567a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import wandb\n",
    "import random\n",
    "import pathlib\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, PreTrainedTokenizerBase\n",
    "\n",
    "from cs336_alignment.data_loading import iterate_batches\n",
    "\n",
    "# Run out of memory running the normal dataset\n",
    "# Create a custom PackedSFTDataset class to load piece of the dataset\n",
    "class PackedSFTDataset(Dataset):\n",
    "    def __init__(self, tokenizer: PreTrainedTokenizerBase, dataset_path: str, seq_length: int, shuffle: bool, max_samples: int = None):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "        with open(dataset_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            raw_data = [json.loads(line) for line in f]\n",
    "\n",
    "        if shuffle:\n",
    "            import random\n",
    "            random.shuffle(raw_data)\n",
    "\n",
    "        if max_samples is not None:\n",
    "            raw_data = raw_data[:max_samples]\n",
    "\n",
    "        self.inputs = []\n",
    "        self.outputs = []\n",
    "        \n",
    "        for ex in raw_data:\n",
    "            sample = (\n",
    "                \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    "                f\"\\n\\n### Instruction:\\n{ex['prompt']}\\n\\n### Response:\\n{ex['response']}\"\n",
    "            )\n",
    "            tokenized = self.tokenizer.encode(sample, truncation=True, max_length=self.seq_length+1)\n",
    "            if len(tokenized) >= 2:\n",
    "                self.inputs.append(tokenized[:-1])\n",
    "                self.outputs.append(tokenized[1:])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = self.inputs[idx]\n",
    "        labels = self.outputs[idx]\n",
    "\n",
    "        input_ids += [self.tokenizer.pad_token_id] * (self.seq_length - len(input_ids))\n",
    "        labels += [-100] * (self.seq_length - len(labels))  # ignore padding in loss\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "            \"labels\": torch.tensor(labels, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Configs\n",
    "OUTPUT_DIR = \"/home/alvin/Homework/s2025-assignment3-alignment/outputs/qwen2.5-3b-sft-2\"\n",
    "PROJECT_NAME = \"EE491B_qwen2.5-3B\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89abebab",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"train_path\": \"/home/alvin/Homework/s2025-assignment3-alignment/data/tuning/safety_augmented_ultrachat_200k_single_turn/train.jsonl\",\n",
    "    \"dev_path\": \"/home/alvin/Homework/s2025-assignment3-alignment/data/tuning/safety_augmented_ultrachat_200k_single_turn/test.jsonl\",\n",
    "    \"output_dir\": OUTPUT_DIR,\n",
    "    \"model_path\": \"/home/alvin/Homework/s2025-assignment3-alignment/models/Qwen/Qwen2.5-3B-Instruct\",\n",
    "    \"vocab_size\": 151936,  # this value depends on the tokenizer used by Qwen2.5\n",
    "    \"context_length\": 2048,\n",
    "    \"d_model\": 2560,\n",
    "    \"num_layers\": 32,\n",
    "    \"num_heads\": 32,\n",
    "    \"d_ff\": 10240,\n",
    "    \"attn_pdrop\": 0.1,\n",
    "    \"residual_pdrop\": 0.1,\n",
    "    \"batch_size\": 1,\n",
    "    \"train_steps\": 20,\n",
    "    \"gradient_accumulation_steps\": 8,\n",
    "    \"eval_iters\": 5,\n",
    "    \"eval_interval\": 10,\n",
    "    \"learning_rate\": 5e-5,\n",
    "    \"lr_scheduler\": \"cosine\",\n",
    "    \"warmup_ratio\": 0.03,\n",
    "    \"weight_decay\": 0.1,\n",
    "    \"adam_beta1\": 0.9,\n",
    "    \"adam_beta2\": 0.95,\n",
    "    \"adam_eps\": 1e-8,\n",
    "    \"grad_clip\": 1.0,\n",
    "    \"device\": \"cuda\",\n",
    "    \"compile\": False,\n",
    "    \"dtype\": \"bfloat16\",\n",
    "    \"wandb_project\": PROJECT_NAME  # Or set to your project name if using Weights & Biases\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0798d81a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">graceful-rain-22</strong> at: <a href='https://wandb.ai/alvinyang101-university-of-hawaii-at-manoa/EE491B_qwen2.5-3B/runs/4rm8lqyi' target=\"_blank\">https://wandb.ai/alvinyang101-university-of-hawaii-at-manoa/EE491B_qwen2.5-3B/runs/4rm8lqyi</a><br> View project at: <a href='https://wandb.ai/alvinyang101-university-of-hawaii-at-manoa/EE491B_qwen2.5-3B' target=\"_blank\">https://wandb.ai/alvinyang101-university-of-hawaii-at-manoa/EE491B_qwen2.5-3B</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250426_180227-4rm8lqyi/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/alvin/Homework/s2025-assignment3-alignment/notebooks/wandb/run-20250426_180530-0a4gc9dr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/alvinyang101-university-of-hawaii-at-manoa/EE491B_qwen2.5-3B/runs/0a4gc9dr' target=\"_blank\">grateful-sun-23</a></strong> to <a href='https://wandb.ai/alvinyang101-university-of-hawaii-at-manoa/EE491B_qwen2.5-3B' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/alvinyang101-university-of-hawaii-at-manoa/EE491B_qwen2.5-3B' target=\"_blank\">https://wandb.ai/alvinyang101-university-of-hawaii-at-manoa/EE491B_qwen2.5-3B</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/alvinyang101-university-of-hawaii-at-manoa/EE491B_qwen2.5-3B/runs/0a4gc9dr' target=\"_blank\">https://wandb.ai/alvinyang101-university-of-hawaii-at-manoa/EE491B_qwen2.5-3B/runs/0a4gc9dr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# WandB Logging\n",
    "wandb.init(project=PROJECT_NAME, config={\n",
    "    \"batch_size\": args[\"batch_size\"],\n",
    "    \"grad_accumulation_steps\": args[\"gradient_accumulation_steps\"],\n",
    "    \"train_steps\": args[\"train_steps\"],\n",
    "    \"learning_rate\": args[\"learning_rate\"],\n",
    "    \"model\": args[\"model_path\"],\n",
    "})\n",
    "\n",
    "\"\"\"\n",
    "Train a language model on one or multiple GPUs.\n",
    "\n",
    "To run single-GPU training:\n",
    "\n",
    "```\n",
    "python scripts/train.py\n",
    "```\n",
    "\n",
    "To run multi-GPU training, use `torchrun`. e.g., for single-node, 2 GPU:\n",
    "\n",
    "```\n",
    "torchrun --standalone --nproc_per_node=2 scripts/train.py\n",
    "```\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import pathlib\n",
    "import sys\n",
    "from contextlib import nullcontext\n",
    "\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "from cs336_basics.data import get_batch\n",
    "from cs336_basics.model import TransformerLM\n",
    "from cs336_basics.optimizer import get_cosine_lr\n",
    "from torch.distributed import destroy_process_group, init_process_group\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from tqdm import tqdm\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def train(\n",
    "    train_path,\n",
    "    dev_path,\n",
    "    output_dir,\n",
    "    model_path,\n",
    "    vocab_size,\n",
    "    context_length,\n",
    "    d_model,\n",
    "    num_layers,\n",
    "    num_heads,\n",
    "    d_ff,\n",
    "    attn_pdrop,\n",
    "    residual_pdrop,\n",
    "    batch_size,\n",
    "    train_steps,\n",
    "    gradient_accumulation_steps,\n",
    "    eval_iters,\n",
    "    eval_interval,\n",
    "    learning_rate,\n",
    "    lr_scheduler,\n",
    "    warmup_ratio,\n",
    "    weight_decay,\n",
    "    adam_beta1,\n",
    "    adam_beta2,\n",
    "    adam_eps,\n",
    "    grad_clip,\n",
    "    device,\n",
    "    compile,\n",
    "    dtype,\n",
    "    wandb_project,\n",
    "):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "    train_data = PackedSFTDataset(tokenizer, train_path, context_length, shuffle=True, max_samples=8192)\n",
    "    dev_data = PackedSFTDataset(tokenizer, dev_path, context_length, shuffle=False, max_samples=4096)\n",
    "    train_loader = iterate_batches(train_data, batch_size=batch_size, shuffle=True)\n",
    "    dev_loader = iterate_batches(dev_data, batch_size=batch_size, shuffle=False)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True, torch_dtype=torch.float32 if torch.cuda.is_available() else torch.float32)\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Wrap model in DDP, if we're using it.\n",
    "    is_ddp = int(os.environ.get(\"RANK\", -1)) != -1\n",
    "    if is_ddp:\n",
    "        init_process_group(backend=\"nccl\")\n",
    "        ddp_rank = int(os.environ[\"RANK\"])\n",
    "        ddp_local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "        ddp_world_size = int(os.environ[\"WORLD_SIZE\"])\n",
    "        device = f\"cuda:{ddp_local_rank}\"\n",
    "        torch.cuda.set_device(device)\n",
    "        seed = ddp_rank  # each process gets a different seed\n",
    "        # Rank 0 does logging, file creation, etc.\n",
    "        is_master_process = ddp_rank == 0\n",
    "    else:\n",
    "        seed = 0\n",
    "        ddp_world_size = 1\n",
    "        is_master_process = True\n",
    "\n",
    "    if is_master_process:\n",
    "        logger.info(\n",
    "            \"Total number of tokens per training step: \"\n",
    "            + str(\n",
    "                gradient_accumulation_steps\n",
    "                * ddp_world_size\n",
    "                * batch_size\n",
    "                * context_length\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Seed each process differently so we can be sure that they\n",
    "    # see different data batches.\n",
    "    # NOTE: This assumes that you're using torch RNG, you may have\n",
    "    # to seed numpy too as well if your code uses numpy random functions.\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # Save the model config\n",
    "    if is_master_process:\n",
    "        model_config_output_path = os.path.join(output_dir, \"model_config.json\")\n",
    "        logger.info(f\"Saving model config to {model_config_output_path}\")\n",
    "        with open(model_config_output_path, \"w\") as f:\n",
    "            json.dump(model.config.to_dict(), f, indent=4)\n",
    "\n",
    "    device_type = \"cuda\" if \"cuda\" in device else \"cpu\"\n",
    "    torch_dtype = {\n",
    "        \"float32\": torch.float32,\n",
    "        \"bfloat16\": torch.bfloat16,\n",
    "        \"float16\": torch.float16,\n",
    "    }[dtype]\n",
    "    if is_master_process:\n",
    "        logger.info(f\"Using dtype: {torch_dtype}\")\n",
    "    amp_ctx = (\n",
    "        nullcontext()\n",
    "        if device_type == \"cpu\"\n",
    "        else torch.amp.autocast(device_type=device_type, dtype=torch_dtype)\n",
    "    )\n",
    "    # GradScaler is only used for FP16\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=(dtype == \"float16\"))\n",
    "\n",
    "    # Move model to the device\n",
    "    model = model.to(device)\n",
    "\n",
    "    # compile the model, requires torch 2.0\n",
    "    if compile:\n",
    "        torch.set_float32_matmul_precision(\"high\")\n",
    "        model = torch.compile(model)\n",
    "\n",
    "    if is_ddp:\n",
    "        model = DDP(model, device_ids=[ddp_local_rank])\n",
    "\n",
    "    # Set up the AdamW optimizer.\n",
    "    # We do not apply decay on 1D parameters (e.g., biases and RMSNorms)\n",
    "    param_dict = {pn: p for pn, p in model.named_parameters() if p.requires_grad}\n",
    "    params_to_decay = [p for _, p in param_dict.items() if p.dim() >= 2]\n",
    "    params_to_not_decay = [p for _, p in param_dict.items() if p.dim() < 2]\n",
    "    optim_groups = [\n",
    "        {\"params\": params_to_decay, \"weight_decay\": weight_decay},\n",
    "        {\"params\": params_to_not_decay, \"weight_decay\": 0.0},\n",
    "    ]\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        optim_groups,\n",
    "        lr=learning_rate,\n",
    "        betas=(adam_beta1, adam_beta2),\n",
    "        eps=adam_eps,\n",
    "    )\n",
    "\n",
    "    # Get the first batch\n",
    "    train_iter = iter(train_loader)\n",
    "    for i in tqdm(range(train_steps)):\n",
    "        for micro_step_idx in range(gradient_accumulation_steps):\n",
    "            try:\n",
    "                batch = next(train_iter)\n",
    "            except StopIteration:\n",
    "                train_iter = iter(train_loader)\n",
    "                batch = next(train_iter)\n",
    "\n",
    "            batch_x = batch[\"input_ids\"].to(device)\n",
    "            batch_y = batch[\"labels\"].to(device)\n",
    "\n",
    "            if is_ddp:\n",
    "                model.require_backward_grad_sync = (micro_step_idx == gradient_accumulation_steps - 1)\n",
    "\n",
    "            with amp_ctx:\n",
    "                outputs = model(batch_x)\n",
    "                logits = outputs.logits\n",
    "                loss = F.cross_entropy(logits.view(-1, logits.size(-1)), batch_y.view(-1))\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "        if grad_clip:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        loss_float = loss.item() * gradient_accumulation_steps\n",
    "        if is_master_process:\n",
    "            logger.info(f\"Train step {i}, Loss: {loss_float}\")\n",
    "            if wandb_project:\n",
    "                wandb.log({\"train_loss\": loss_float, \"lr\": lr}, step=i)\n",
    "\n",
    "        if i != 0 and i % eval_interval == 0 and is_master_process:\n",
    "            dev_loss = estimate_dev_loss(\n",
    "                model=model,\n",
    "                dev_loader=dev_loader,\n",
    "                eval_iters=eval_iters,\n",
    "                device=device,\n",
    "            )\n",
    "            logger.info(f\"Estimated validation loss: {dev_loss}\")\n",
    "            if wandb_project:\n",
    "                wandb.log({\"eval_loss\": dev_loss}, step=i)\n",
    "\n",
    "    # Calculate final estimated dev loss\n",
    "    if is_master_process:\n",
    "        dev_loss = estimate_dev_loss(\n",
    "            model=model,\n",
    "            dev_loader=dev_loader,\n",
    "            eval_iters=eval_iters,\n",
    "            device=device,\n",
    "        )\n",
    "        logger.info(f\"Final estimated validation loss: {dev_loss}\")\n",
    "        if wandb_project:\n",
    "            wandb.log({\"eval_loss\": dev_loss}, step=train_steps)\n",
    "        # Save the model weights\n",
    "        model_weights_output_path = os.path.join(output_dir, \"model.pt\")\n",
    "        logger.info(f\"Saving model weights to {model_weights_output_path}\")\n",
    "        torch.save(model.state_dict(), model_weights_output_path)\n",
    "\n",
    "    if is_ddp:\n",
    "        destroy_process_group()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_dev_loss(\n",
    "    model: TransformerLM,\n",
    "    dev_loader: torch.utils.data.DataLoader,\n",
    "    eval_iters: int,\n",
    "    device: str,\n",
    "):\n",
    "    model.eval()\n",
    "    losses = torch.zeros(eval_iters)\n",
    "    dev_iter = iter(dev_loader)\n",
    "    for k in tqdm(range(eval_iters)):\n",
    "        try:\n",
    "            batch = next(dev_iter)\n",
    "        except StopIteration:\n",
    "            dev_iter = iter(dev_loader)\n",
    "            batch = next(dev_iter)\n",
    "\n",
    "        batch_x = batch[\"input_ids\"].to(device)\n",
    "        batch_y = batch[\"labels\"].to(device)\n",
    "\n",
    "        logits = model(batch_x)\n",
    "        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), batch_y.view(-1))\n",
    "        losses[k] = loss.item()\n",
    "    model.train()\n",
    "    return losses.mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c2a7c4c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 25.34it/s]\n",
      "/tmp/ipykernel_12773/3820800916.py:144: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == \"float16\"))\n",
      "  0%|          | 0/20 [09:11<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 86.00 MiB. GPU 0 has a total capacity of 47.99 GiB of which 0 bytes is free. Process 9861 has 17179869184.00 GiB memory in use. Process 46735 has 17179869184.00 GiB memory in use. Process 17381 has 17179869184.00 GiB memory in use. Of the allocated memory 108.84 GiB is allocated by PyTorch, and 1.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[15], line 200\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(train_path, dev_path, output_dir, model_path, vocab_size, context_length, d_model, num_layers, num_heads, d_ff, attn_pdrop, residual_pdrop, batch_size, train_steps, gradient_accumulation_steps, eval_iters, eval_interval, learning_rate, lr_scheduler, warmup_ratio, weight_decay, adam_beta1, adam_beta2, adam_eps, grad_clip, device, compile, dtype, wandb_project)\u001b[0m\n\u001b[1;32m    197\u001b[0m     scaler\u001b[38;5;241m.\u001b[39munscale_(optimizer)\n\u001b[1;32m    198\u001b[0m     torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), grad_clip)\n\u001b[0;32m--> 200\u001b[0m scaler\u001b[38;5;241m.\u001b[39mstep(optimizer)\n\u001b[1;32m    201\u001b[0m scaler\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[1;32m    202\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/cs336_alignment/lib/python3.10/site-packages/torch/amp/grad_scaler.py:380\u001b[0m, in \u001b[0;36mGradScaler.step\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Invoke ``unscale_(optimizer)`` followed by parameter update, if gradients are not infs/NaN.\u001b[39;00m\n\u001b[1;32m    359\u001b[0m \n\u001b[1;32m    360\u001b[0m \u001b[38;5;124;03m:meth:`step` carries out the following two operations:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;124;03m    Closure use is not currently supported.\u001b[39;00m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enabled:\n\u001b[0;32m--> 380\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosure\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    384\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClosure use is not currently supported if GradScaler is enabled.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    385\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/cs336_alignment/lib/python3.10/site-packages/torch/optim/optimizer.py:493\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    489\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    490\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 493\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    496\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/cs336_alignment/lib/python3.10/site-packages/torch/optim/optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/miniconda3/envs/cs336_alignment/lib/python3.10/site-packages/torch/optim/adamw.py:243\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    230\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m cast(Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m], group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    232\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    233\u001b[0m         group,\n\u001b[1;32m    234\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    240\u001b[0m         state_steps,\n\u001b[1;32m    241\u001b[0m     )\n\u001b[0;32m--> 243\u001b[0m     \u001b[43madamw\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/miniconda3/envs/cs336_alignment/lib/python3.10/site-packages/torch/optim/optimizer.py:154\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cs336_alignment/lib/python3.10/site-packages/torch/optim/adamw.py:875\u001b[0m, in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    872\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    873\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adamw\n\u001b[0;32m--> 875\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cs336_alignment/lib/python3.10/site-packages/torch/optim/adamw.py:699\u001b[0m, in \u001b[0;36m_multi_tensor_adamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, has_complex)\u001b[0m\n\u001b[1;32m    697\u001b[0m     exp_avg_sq_sqrt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_foreach_sqrt(device_max_exp_avg_sqs)\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 699\u001b[0m     exp_avg_sq_sqrt \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_foreach_sqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_exp_avg_sqs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    701\u001b[0m torch\u001b[38;5;241m.\u001b[39m_foreach_div_(exp_avg_sq_sqrt, bias_correction2_sqrt)\n\u001b[1;32m    702\u001b[0m torch\u001b[38;5;241m.\u001b[39m_foreach_add_(exp_avg_sq_sqrt, eps)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 86.00 MiB. GPU 0 has a total capacity of 47.99 GiB of which 0 bytes is free. Process 9861 has 17179869184.00 GiB memory in use. Process 46735 has 17179869184.00 GiB memory in use. Process 17381 has 17179869184.00 GiB memory in use. Of the allocated memory 108.84 GiB is allocated by PyTorch, and 1.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "train(**args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs336_alignment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
